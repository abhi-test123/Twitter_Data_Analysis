{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling and Analysis project\n",
    "> Authored by: **Abhishek Pandey** [LinkedIn](https://www.linkedin.com/in/abhishekpandeyit/) | [Twitter](https://twitter.com/PandeyJii_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "> This project is contains the wrangling and analysis  or Archived chats of [Twitter](https://twitter.com/PandeyJii_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data\n",
    "> 1. **Twitter archive file:** Loading this [file](twitter_archive_enhanced.csv) into our dataframe using pandas\n",
    ">\n",
    ">2. **The tweet image predictions:** This find contains informations such as which breed of dog (or other object, animal, etc.) is present in each tweet. This file was provide at [Udacity](https://www.udacity.com), and as mentioned in the rubrics I downloaded this file using **Requests library**\n",
    ">\n",
    ">3. **Twitter API & JSON:** Using the tweet IDs in the Twitter archive, querying the Twitter API for each tweet is done by creating a developer account at Twitter and using some secret keys. Then read this file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count. \n",
    ">\n",
    "### This template to gather data from Twitter API is provided by [Udacity](www.Udacity.com). I am just using that template as mentioned in the Udacity workspace using my secret token and access tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required libraries\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import re\n",
    "import requests #For gathering data from website\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = pd.read_csv('F:/Github_repo/Wrangle_Analyse_Dataset/twitter-archive-enhanced-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.sort_values('timestamp')\n",
    "df_tweet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "response = requests.get(url)\n",
    "with open('image_predictions.tsv', mode ='wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.read_csv('F:/Github_repo/Wrangle_Analyse_Dataset/image_predictions.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = '0Z0ZK59KSOUWYr1X7qIRY9NHx'\n",
    "consumer_secret = 'QxaSSKis3VdPIrIjdGhuGEa3DYO0i3LbyTuOwixWcA3cFScgqC'\n",
    "access_token = '1257973204804612097-SFMNLhInnBpMfX3lUva4jNLTUp5PoS'\n",
    "access_secret = 'aeSo4r2jwuE6yKbwpnVH0XzUvgwdl9e7ZBheiowEIsqus'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = list(df_tweet['tweet_id'])\n",
    "\n",
    "tweet_data = []\n",
    "tweet_id_success = []\n",
    "tweet_id_missing = []\n",
    "for tweet_id in tweet_ids:\n",
    "    try:\n",
    "        data = api.get_status(tweet_id, \n",
    "                              tweet_mode='extended',\n",
    "                              wait_on_rate_limit = True,\n",
    "                              wait_on_rate_limit_notify = True)\n",
    "        tweet_data.append(data)\n",
    "        tweet_id_success.append(tweet_id)\n",
    "    except:\n",
    "        tweet_id_missing.append(tweet_id)\n",
    "        print(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total Tweets = {len(tweet_data)} and Success Ids={len(tweet_id_success)} / Missing Ids={len(tweet_id_missing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then in this code block we isolate the json part of each tweepy \n",
    "#status object that we have downloaded and we add them all into a list\n",
    "\n",
    "my_list_of_dicts = []\n",
    "for each_json_tweet in list_of_tweets:\n",
    "    my_list_of_dicts.append(each_json_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify information of interest from JSON dictionaries in txt file\n",
    "#and put it in a dataframe called tweet JSON\n",
    "my_demo_list = []\n",
    "with open('tweet-json.txt', encoding='utf-8') as json_file:  \n",
    "    all_data = json.load(json_file)\n",
    "    for each_dictionary in all_data:\n",
    "        tweet_id = each_dictionary['id']\n",
    "        whole_tweet = each_dictionary['text']\n",
    "        only_url = whole_tweet[whole_tweet.find('https'):]\n",
    "        favorite_count = each_dictionary['favorite_count']\n",
    "        retweet_count = each_dictionary['retweet_count']\n",
    "        followers_count = each_dictionary['user']['followers_count']\n",
    "        friends_count = each_dictionary['user']['friends_count']\n",
    "        whole_source = each_dictionary['source']\n",
    "        only_device = whole_source[whole_source.find('rel=\"nofollow\">') + 15:-4]\n",
    "        source = only_device\n",
    "        retweeted_status = each_dictionary['retweeted_status'] = each_dictionary.get('retweeted_status', 'Original tweet')\n",
    "        if retweeted_status == 'Original tweet':\n",
    "            url = only_url\n",
    "        else:\n",
    "            retweeted_status = 'This is a retweet'\n",
    "            url = 'This is a retweet'\n",
    "\n",
    "        my_demo_list.append({'tweet_id': str(tweet_id),\n",
    "                             'favorite_count': int(favorite_count),\n",
    "                             'retweet_count': int(retweet_count),\n",
    "                             'followers_count': int(followers_count),\n",
    "                             'friends_count': int(friends_count),\n",
    "                             'url': url,\n",
    "                             'source': source,\n",
    "                             'retweeted_status': retweeted_status,\n",
    "                            })\n",
    "        tweet_json = pd.DataFrame(my_demo_list, columns = ['tweet_id', 'favorite_count','retweet_count', \n",
    "                                                           'followers_count', 'friends_count','source', \n",
    "                                                           'retweeted_status', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Twitter API data\n",
    "df_new = pd.read_json('F:\\\\Github_repo\\\\Wrangle_Analyse_Dataset\\\\tweet-json.txt', encoding = 'utf-8', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['retweeted_status_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['retweeted_status_user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['retweeted_status_timestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['expanded_urls'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['doggo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['floofer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['pupper'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['puppo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.loc[df_tweet['name'].str.isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['tweet_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['favorite_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['retweet_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['tweet_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['jpg_url'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['img_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p1_conf'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p1_dog'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p2_conf'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p2_dog'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p3_conf'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['p3_dog'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of the dataframes for cleaning\n",
    "\n",
    "twitter1_clean = df_tweet.copy()\n",
    "twitter2_clean = df_new.copy()\n",
    "images_clean = images.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter2 and images data must combined with twitter1 data as they contain information about the same tweet, therefore merging these 3 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merging twitter1, twitter2, and images dataframes with 'tweet_id''''\n",
    "dfs = [twitter1_clean, twitter2_clean, images_clean]\n",
    "twitter = reduce(lambda left,right: pd.merge(left,right,on='tweet_id'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find another issue i.e. 1 variable (dog stage) in 4 different columns (doggo, floofer, pupper, and puppo), to solve this issue extracting the dog stages from the 'text' column into the new 'dog_stage' column using regular expression then droping the 'doggo', 'floofer', 'pupper', and 'puppo' columns.  This also takes care of the quality issue of \"name has values that are the string \"None\" instead of NaN\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter['dog_stage'] = twitter['text'].str.extract('(doggo|floofer|pupper|puppo)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter[['dog_stage','doggo', 'floofer', 'pupper', 'puppo']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis=1) #Dropping the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next issue needs to be readdressed is that data contains retweets. To solve this issue Rows where 'retweeted_status_id' is a NaN will be kept (ie. if it has a value it will be removed), then the 'retweeted_status_id', 'retweeted_status_user_id' and 'retweeted_status_timestamp' columns will be removed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter[np.isnan(twitter.retweeted_status_id)]\n",
    "\n",
    "print(twitter.info())\n",
    "\n",
    "twitter = twitter.drop(['retweeted_status_id', \n",
    "                        'retweeted_status_user_id', \n",
    "                        'retweeted_status_timestamp'], \n",
    "                       axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting tweet_id into string type\n",
    "twitter['tweet_id'] = twitter['tweet_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'timestamp' and 'retweeted_status_timestamp' are currently of type 'object' therefore Converting 'timestamp' to a datetime object ('retweeted_status_timestamp' was deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter['timestamp'] = twitter['timestamp'].str.slice(start=0, stop=-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter['timestamp'] = pd.to_datetime(twitter['timestamp'], format = \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I found several issues\n",
    "- 1. name has values that are the string \"None\" instead of NaN\n",
    "- 2. Looking programmatically, some names are inaccurate such as \"a\", \"an\", \"the\", \"very\", \"by\", etc.  Looking visually in Excel, I was able to find more names that are inaccurate including \"actually\", \"quite\", \"unacceptable\", \"mad\", \"not\" and \"old.  It seems like the method used to extract the names was using the word the followed \"This is...\" and \"Here is...\" which leads to some inaccuracies. \n",
    "- 3. I also found an instand of a name being \"O\" instead of \"O'Malley\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Replacing all inaccurate names with NaNs, and the \"O\" with \"O'Malley\".Find all names that start with a lowercase letter'''\n",
    "lowercase_names = []\n",
    "for row in twitter['name']:\n",
    "    if row[0].islower() and row not in lowercase_names:\n",
    "        lowercase_names.append(row)\n",
    "print(lowercase_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter['name'].replace(lowercase_names, \n",
    "                        np.nan,\n",
    "                       inplace = True)\n",
    "\n",
    "twitter['name'].replace('None', \n",
    "                        np.nan,\n",
    "                       inplace = True)\n",
    "\n",
    "twitter['name'].replace('O', \n",
    "                        \"O'Malley\",\n",
    "                       inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find 1 more issue i.e.  Some ratings with decimals such as 13.5/10, 9.5/10 have been incorrectly exported as 5/10 (in addition to other numbers with decimals such as 11.26 and 11.27). therefore Finding all instances of ratings that contained decimals and replace the numerator values with the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Obtain all text, indices, and ratings for tweets that contain a decimal in the numerator of rating\n",
    "'''\n",
    "ratings_with_decimals_text = []\n",
    "ratings_with_decimals_index = []\n",
    "ratings_with_decimals = []\n",
    "\n",
    "for i, text in twitter['text'].iteritems():\n",
    "    if bool(re.search('\\d+\\.\\d+\\/\\d+', text)):\n",
    "        ratings_with_decimals_text.append(text)\n",
    "        ratings_with_decimals_index.append(i)\n",
    "        ratings_with_decimals.append(re.search('\\d+\\.\\d+', text).group())\n",
    "\n",
    "# Print the text to confirm presence of ratings with decimals        \n",
    "ratings_with_decimals_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing indexs of text with decimal ratings\n",
    "ratings_with_decimals_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.loc[ratings_with_decimals_index[0],'rating_numerator'] = float(ratings_with_decimals[0])\n",
    "twitter.loc[ratings_with_decimals_index[1],'rating_numerator'] = float(ratings_with_decimals[1])\n",
    "twitter.loc[ratings_with_decimals_index[2],'rating_numerator'] = float(ratings_with_decimals[2])\n",
    "twitter.loc[ratings_with_decimals_index[3],'rating_numerator'] = float(ratings_with_decimals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.loc[40] # Check contents of row with index 40 to ensure the rating is corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For future analysis it could be confusing to interpret unstandardized ratings.  It is their gimmick to give dogs a rating of 100% but not all are above 100% so it could be interesting to see what percentage are below or above 100 and how this has been changed overtime by calculating a single value for rating, to solve this issue calulating the value of the numerator divided by the denominator and save this in a new column called 'rating'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter['rating'] = twitter['rating_numerator'] / twitter['rating_denominator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the presence of column and calculation\n",
    "twitter.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many columns in this dataframe making it hard to read, and some will not be needed for analysis.  In addition some of the text in the table output is cut off.\n",
    "<br>**Solution**: Drop undesired columns and change table display settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing all columns name\n",
    "twitter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.rename(columns={'rating_numerator': 'numerator', \n",
    "                        'rating_denominator': 'denominator'}, inplace=True) #Shortning column names\n",
    "\n",
    "'''\n",
    "Droping unwanted columns\n",
    "'''\n",
    "twitter.drop(['in_reply_to_status_id', \n",
    "              'in_reply_to_user_id',\n",
    "              'source',\n",
    "              'img_num'], axis=1, inplace=True)\n",
    "\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming changes\n",
    "twitter.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing dataframe to a new file.\n",
    "twitter.to_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"retweet_count\", \n",
    "           y=\"favorite_count\", \n",
    "           data=twitter,\n",
    "           size = 5,\n",
    "           aspect=1.3,\n",
    "           scatter_kws={'alpha':1/5})\n",
    "plt.title('Favorite Tweets vs. Retweet Count')\n",
    "plt.xlabel('Retweet Count')\n",
    "plt.ylabel('Favorite Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs shows a strong positive correlation between Favorite and retweet counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.groupby('timestamp')['rating'].mean().plot(kind='line')\n",
    "plt.title('Rating over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Standardized Rating')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 3 outliers with a rating over 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.loc[twitter['rating'] > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inaccurate finding (rating of 24/7) found here at the first instance. I decided to leave this unfixed as we can easily ignore this. The other 2 tweets are accurate ratings i.e jokes. Thus I am interestedto find that if the tweets with a standardized rating below than 1, decrease over time or not, therefore I'm limiting the y axis from 0 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting standardized ratings over time with ylim of 0-2 to find accurate answers\n",
    "\"\"\"\n",
    "\n",
    "twitter.groupby('timestamp')['rating'].mean().plot(kind='line')\n",
    "plt.ylim(0, 2)\n",
    "plt.title('Rating over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Standardized Rating')\n",
    "plt.show;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}